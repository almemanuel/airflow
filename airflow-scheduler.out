[[34m2022-11-16 14:34:35,335[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[[34m2022-11-16 14:34:35,336[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[[34m2022-11-16 14:34:35,337[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2022-11-16 14:34:35,340[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 9817[0m
[[34m2022-11-16 14:34:35,341[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 14:34:35,343[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-11-16T14:34:35.355-0300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2022-11-16 14:39:35,544[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 14:44:35,610[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 14:49:35,784[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 14:54:35,950[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 14:59:36,248[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:10:43,252[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:15:43,400[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:20:43,646[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:22:27,573[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for tutorial to 2022-11-16T18:22:11.450432+00:00, run_after=2022-11-17T18:22:11.450432+00:00[0m
[[34m2022-11-16 15:22:27,621[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun tutorial @ 2022-11-15 18:22:11.450432+00:00: scheduled__2022-11-15T18:22:11.450432+00:00, state:running, queued_at: 2022-11-16 18:22:27.564337+00:00. externally triggered: False> successful[0m
[[34m2022-11-16 15:22:27,623[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=tutorial, execution_date=2022-11-15 18:22:11.450432+00:00, run_id=scheduled__2022-11-15T18:22:11.450432+00:00, run_start_date=2022-11-16 18:22:27.590287+00:00, run_end_date=2022-11-16 18:22:27.623204+00:00, run_duration=0.032917, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2022-11-15 18:22:11.450432+00:00, data_interval_end=2022-11-16 18:22:11.450432+00:00, dag_hash=d8e3f444be770dca34929116d8f1a522[0m
[[34m2022-11-16 15:22:27,625[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for tutorial to 2022-11-16T18:22:11.450432+00:00, run_after=2022-11-17T18:22:11.450432+00:00[0m
[[34m2022-11-16 15:22:27,656[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: tutorial.print_date manual__2022-11-16T18:22:26.684389+00:00 [scheduled]>[0m
[[34m2022-11-16 15:22:27,656[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG tutorial has 0/16 running and queued tasks[0m
[[34m2022-11-16 15:22:27,656[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: tutorial.print_date manual__2022-11-16T18:22:26.684389+00:00 [scheduled]>[0m
[[34m2022-11-16 15:22:27,658[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='tutorial', task_id='print_date', run_id='manual__2022-11-16T18:22:26.684389+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2022-11-16 15:22:27,658[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'tutorial', 'print_date', 'manual__2022-11-16T18:22:26.684389+00:00', '--local', '--subdir', '/home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py'][0m
[[34m2022-11-16 15:22:27,665[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'tutorial', 'print_date', 'manual__2022-11-16T18:22:26.684389+00:00', '--local', '--subdir', '/home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py'][0m
[[34m2022-11-16 15:22:28,790[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py[0m
[[34m2022-11-16 15:22:29,108[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: tutorial.print_date manual__2022-11-16T18:22:26.684389+00:00 [queued]> on host PGON_GAR_TI11.princesadoscampos.local[0m
[[34m2022-11-16 15:22:29,760[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of tutorial.print_date run_id=manual__2022-11-16T18:22:26.684389+00:00 exited with status success for try_number 1[0m
[[34m2022-11-16 15:22:29,809[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=tutorial, task_id=print_date, run_id=manual__2022-11-16T18:22:26.684389+00:00, map_index=-1, run_start_date=2022-11-16 18:22:29.257146+00:00, run_end_date=2022-11-16 18:22:29.489815+00:00, run_duration=0.232669, state=success, executor_state=success, try_number=1, max_tries=1, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2022-11-16 18:22:27.656671+00:00, queued_by_job_id=2, pid=12104[0m
[[34m2022-11-16 15:22:29,960[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 2 tasks up for execution:
	<TaskInstance: tutorial.sleep manual__2022-11-16T18:22:26.684389+00:00 [scheduled]>
	<TaskInstance: tutorial.templated manual__2022-11-16T18:22:26.684389+00:00 [scheduled]>[0m
[[34m2022-11-16 15:22:29,960[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG tutorial has 0/16 running and queued tasks[0m
[[34m2022-11-16 15:22:29,960[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG tutorial has 1/16 running and queued tasks[0m
[[34m2022-11-16 15:22:29,960[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: tutorial.sleep manual__2022-11-16T18:22:26.684389+00:00 [scheduled]>
	<TaskInstance: tutorial.templated manual__2022-11-16T18:22:26.684389+00:00 [scheduled]>[0m
[[34m2022-11-16 15:22:29,962[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='tutorial', task_id='sleep', run_id='manual__2022-11-16T18:22:26.684389+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-11-16 15:22:29,962[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'tutorial', 'sleep', 'manual__2022-11-16T18:22:26.684389+00:00', '--local', '--subdir', '/home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py'][0m
[[34m2022-11-16 15:22:29,962[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='tutorial', task_id='templated', run_id='manual__2022-11-16T18:22:26.684389+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-11-16 15:22:29,962[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'tutorial', 'templated', 'manual__2022-11-16T18:22:26.684389+00:00', '--local', '--subdir', '/home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py'][0m
[[34m2022-11-16 15:22:29,968[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'tutorial', 'sleep', 'manual__2022-11-16T18:22:26.684389+00:00', '--local', '--subdir', '/home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py'][0m
[[34m2022-11-16 15:22:30,446[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py[0m
[[34m2022-11-16 15:22:30,615[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: tutorial.sleep manual__2022-11-16T18:22:26.684389+00:00 [queued]> on host PGON_GAR_TI11.princesadoscampos.local[0m
[[34m2022-11-16 15:22:36,041[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'tutorial', 'templated', 'manual__2022-11-16T18:22:26.684389+00:00', '--local', '--subdir', '/home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py'][0m
[[34m2022-11-16 15:22:36,571[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/emanuel/.local/share/virtualenvs/apache-airflow-EbdR2nYd/lib/python3.8/site-packages/airflow/example_dags/tutorial.py[0m
[[34m2022-11-16 15:22:36,755[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: tutorial.templated manual__2022-11-16T18:22:26.684389+00:00 [queued]> on host PGON_GAR_TI11.princesadoscampos.local[0m
[[34m2022-11-16 15:22:37,152[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of tutorial.sleep run_id=manual__2022-11-16T18:22:26.684389+00:00 exited with status success for try_number 1[0m
[[34m2022-11-16 15:22:37,153[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of tutorial.templated run_id=manual__2022-11-16T18:22:26.684389+00:00 exited with status success for try_number 1[0m
[[34m2022-11-16 15:22:37,158[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=tutorial, task_id=sleep, run_id=manual__2022-11-16T18:22:26.684389+00:00, map_index=-1, run_start_date=2022-11-16 18:22:30.676281+00:00, run_end_date=2022-11-16 18:22:35.827119+00:00, run_duration=5.150838, state=success, executor_state=success, try_number=1, max_tries=3, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-11-16 18:22:29.961107+00:00, queued_by_job_id=2, pid=12109[0m
[[34m2022-11-16 15:22:37,159[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=tutorial, task_id=templated, run_id=manual__2022-11-16T18:22:26.684389+00:00, map_index=-1, run_start_date=2022-11-16 18:22:36.816695+00:00, run_end_date=2022-11-16 18:22:36.957132+00:00, run_duration=0.140437, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-11-16 18:22:29.961107+00:00, queued_by_job_id=2, pid=12114[0m
[[34m2022-11-16 15:22:37,314[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun tutorial @ 2022-11-16 18:22:26.684389+00:00: manual__2022-11-16T18:22:26.684389+00:00, state:running, queued_at: 2022-11-16 18:22:26.746427+00:00. externally triggered: True> successful[0m
[[34m2022-11-16 15:22:37,315[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=tutorial, execution_date=2022-11-16 18:22:26.684389+00:00, run_id=manual__2022-11-16T18:22:26.684389+00:00, run_start_date=2022-11-16 18:22:27.590647+00:00, run_end_date=2022-11-16 18:22:37.315259+00:00, run_duration=9.724612, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-11-15 18:22:26.684389+00:00, data_interval_end=2022-11-16 18:22:26.684389+00:00, dag_hash=d8e3f444be770dca34929116d8f1a522[0m
[[34m2022-11-16 15:22:37,317[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for tutorial to 2022-11-16T18:22:26.684389+00:00, run_after=2022-11-17T18:22:26.684389+00:00[0m
[[34m2022-11-16 15:25:43,792[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:30:44,050[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:35:44,196[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:40:44,588[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:45:44,950[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:50:45,131[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:55:45,519[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 15:56:54,322[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: fluxo_simples.print_date manual__2022-11-16T18:56:53.823627+00:00 [scheduled]>[0m
[[34m2022-11-16 15:56:54,322[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG fluxo_simples has 0/16 running and queued tasks[0m
[[34m2022-11-16 15:56:54,322[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: fluxo_simples.print_date manual__2022-11-16T18:56:53.823627+00:00 [scheduled]>[0m
[[34m2022-11-16 15:56:54,329[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='fluxo_simples', task_id='print_date', run_id='manual__2022-11-16T18:56:53.823627+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-11-16 15:56:54,329[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fluxo_simples', 'print_date', 'manual__2022-11-16T18:56:53.823627+00:00', '--local', '--subdir', 'DAGS_FOLDER/first_script.py'][0m
[[34m2022-11-16 15:56:54,336[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'fluxo_simples', 'print_date', 'manual__2022-11-16T18:56:53.823627+00:00', '--local', '--subdir', 'DAGS_FOLDER/first_script.py'][0m
[[34m2022-11-16 15:56:57,292[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/emanuel/apache-airflow/dags/first_script.py[0m
[[34m2022-11-16 15:56:57,659[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: fluxo_simples.print_date manual__2022-11-16T18:56:53.823627+00:00 [queued]> on host PGON_GAR_TI11.princesadoscampos.local[0m
[[34m2022-11-16 15:56:58,361[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of fluxo_simples.print_date run_id=manual__2022-11-16T18:56:53.823627+00:00 exited with status success for try_number 1[0m
[[34m2022-11-16 15:56:58,378[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=fluxo_simples, task_id=print_date, run_id=manual__2022-11-16T18:56:53.823627+00:00, map_index=-1, run_start_date=2022-11-16 18:56:57.785783+00:00, run_end_date=2022-11-16 18:56:58.042153+00:00, run_duration=0.25637, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2022-11-16 18:56:54.323276+00:00, queued_by_job_id=2, pid=15219[0m
[[34m2022-11-16 15:56:58,668[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: fluxo_simples.sleep manual__2022-11-16T18:56:53.823627+00:00 [scheduled]>[0m
[[34m2022-11-16 15:56:58,669[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG fluxo_simples has 0/16 running and queued tasks[0m
[[34m2022-11-16 15:56:58,669[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: fluxo_simples.sleep manual__2022-11-16T18:56:53.823627+00:00 [scheduled]>[0m
[[34m2022-11-16 15:56:58,670[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='fluxo_simples', task_id='sleep', run_id='manual__2022-11-16T18:56:53.823627+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-11-16 15:56:58,670[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fluxo_simples', 'sleep', 'manual__2022-11-16T18:56:53.823627+00:00', '--local', '--subdir', 'DAGS_FOLDER/first_script.py'][0m
[[34m2022-11-16 15:56:58,675[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'fluxo_simples', 'sleep', 'manual__2022-11-16T18:56:53.823627+00:00', '--local', '--subdir', 'DAGS_FOLDER/first_script.py'][0m
[[34m2022-11-16 15:56:59,278[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/emanuel/apache-airflow/dags/first_script.py[0m
[[34m2022-11-16 15:56:59,531[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: fluxo_simples.sleep manual__2022-11-16T18:56:53.823627+00:00 [queued]> on host PGON_GAR_TI11.princesadoscampos.local[0m
[[34m2022-11-16 15:57:05,199[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of fluxo_simples.sleep run_id=manual__2022-11-16T18:56:53.823627+00:00 exited with status success for try_number 1[0m
[[34m2022-11-16 15:57:05,207[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=fluxo_simples, task_id=sleep, run_id=manual__2022-11-16T18:56:53.823627+00:00, map_index=-1, run_start_date=2022-11-16 18:56:59.640038+00:00, run_end_date=2022-11-16 18:57:04.945590+00:00, run_duration=5.305552, state=success, executor_state=success, try_number=1, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2022-11-16 18:56:58.669546+00:00, queued_by_job_id=2, pid=15224[0m
[[34m2022-11-16 15:57:05,499[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: fluxo_simples.save_date manual__2022-11-16T18:56:53.823627+00:00 [scheduled]>[0m
[[34m2022-11-16 15:57:05,499[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG fluxo_simples has 0/16 running and queued tasks[0m
[[34m2022-11-16 15:57:05,499[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: fluxo_simples.save_date manual__2022-11-16T18:56:53.823627+00:00 [scheduled]>[0m
[[34m2022-11-16 15:57:05,501[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='fluxo_simples', task_id='save_date', run_id='manual__2022-11-16T18:56:53.823627+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-11-16 15:57:05,501[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fluxo_simples', 'save_date', 'manual__2022-11-16T18:56:53.823627+00:00', '--local', '--subdir', 'DAGS_FOLDER/first_script.py'][0m
[[34m2022-11-16 15:57:05,507[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'fluxo_simples', 'save_date', 'manual__2022-11-16T18:56:53.823627+00:00', '--local', '--subdir', 'DAGS_FOLDER/first_script.py'][0m
[[34m2022-11-16 15:57:06,154[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/emanuel/apache-airflow/dags/first_script.py[0m
[[34m2022-11-16 15:57:06,384[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: fluxo_simples.save_date manual__2022-11-16T18:56:53.823627+00:00 [queued]> on host PGON_GAR_TI11.princesadoscampos.local[0m
[[34m2022-11-16 15:57:06,901[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of fluxo_simples.save_date run_id=manual__2022-11-16T18:56:53.823627+00:00 exited with status success for try_number 1[0m
[[34m2022-11-16 15:57:06,908[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=fluxo_simples, task_id=save_date, run_id=manual__2022-11-16T18:56:53.823627+00:00, map_index=-1, run_start_date=2022-11-16 18:57:06.466710+00:00, run_end_date=2022-11-16 18:57:06.626342+00:00, run_duration=0.159632, state=success, executor_state=success, try_number=1, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2022-11-16 18:57:05.499979+00:00, queued_by_job_id=2, pid=15229[0m
[[34m2022-11-16 15:57:07,164[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun fluxo_simples @ 2022-11-16 18:56:53.823627+00:00: manual__2022-11-16T18:56:53.823627+00:00, state:running, queued_at: 2022-11-16 18:56:53.872768+00:00. externally triggered: True> successful[0m
[[34m2022-11-16 15:57:07,164[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=fluxo_simples, execution_date=2022-11-16 18:56:53.823627+00:00, run_id=manual__2022-11-16T18:56:53.823627+00:00, run_start_date=2022-11-16 18:56:54.272743+00:00, run_end_date=2022-11-16 18:57:07.164525+00:00, run_duration=12.891782, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-11-15 18:56:53.823627+00:00, data_interval_end=2022-11-16 18:56:53.823627+00:00, dag_hash=cde2990712c81b83a8539e23774612bf[0m
[[34m2022-11-16 15:57:07,167[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for fluxo_simples to 2022-11-16T18:56:53.823627+00:00, run_after=2022-11-17T18:56:53.823627+00:00[0m
[[34m2022-11-16 16:00:45,787[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-11-16 16:05:46,054[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
